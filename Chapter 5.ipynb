{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfd13a0",
   "metadata": {},
   "source": [
    "# Deep Learning Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756e104",
   "metadata": {},
   "source": [
    "## Layers and Blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b9bff",
   "metadata": {},
   "source": [
    "A block could describe a single layer, a component consisting of multiple layers, or the entire model itself.  \n",
    "**Benefit**: Combining multiple layers into blocks can implement complex networks.  \n",
    "\n",
    "When programming, we need to worry about **parameters** and **the forward propagation function**.  \n",
    "\n",
    "**A Custom Block**\n",
    "The basic functionality that each block must provide:\n",
    "1. Ingest input data as arguments to its forward propagation function.\n",
    "2. Generate an output by having the forward propagation function return a value. Note that the output may have a different shape from the input. For example, the first fully-connected layer in our model above ingests an input of dimension 20 but returns an output of dimension 256.\n",
    "3. Calculate the gradient of its output with respect to its input, which can be accessed via its back propagation function. Typically this happens automatically.\n",
    "4. Store and provide access to those parameters necessary to execute the forward propagation computation.\n",
    "5. Initialize model parameters as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c74287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256601a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1266, -0.1119, -0.0194,  0.0367,  0.0129, -0.1256, -0.0122,  0.0760,\n",
       "          0.0910,  0.0641],\n",
       "        [-0.1256, -0.1306,  0.1358,  0.1173, -0.0057, -0.0514, -0.0688,  0.0318,\n",
       "          0.0220,  0.0721]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21337914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644693d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1416,  0.0397,  0.2359,  0.0339,  0.0709, -0.1745,  0.0223, -0.0975,\n",
       "          0.0793, -0.1087],\n",
       "        [ 0.0816,  0.0416,  0.1981,  0.0437,  0.0139, -0.1104, -0.0383, -0.0727,\n",
       "          0.2010, -0.2099]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb067afa",
   "metadata": {},
   "source": [
    "Show how the **Sequential** class works.  \n",
    "If we want to implement a new **MySequential** class, which has the same functionality with the default **Sequential** class, we need to define two key function: \n",
    "1. A function to append blocks one by one to a list. \n",
    "2. A forward propagation function to pass an input through the chain of blocks, in the same order as they were appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bac018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # Here, `module` is an instance of a `Module` subclass. We save it\n",
    "            # in the member variable `_modules` of the `Module` class, and its\n",
    "            # type is OrderedDict\n",
    "            # Advantage: during our module ºs parameter initialization, \n",
    "            # the system knows to look inside the _modules dictionary to find \n",
    "            # sub-modules whose parameters also need to be initialized.\n",
    "            self._modules[str(idx)] = module\n",
    "    def forward(self, X):\n",
    "        # OrderedDict guarantees that the members will be traversed in the order they added\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8459a1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1100,  0.0394, -0.0080, -0.1859, -0.1853, -0.0766,  0.1514, -0.2393,\n",
       "         -0.1976, -0.0616],\n",
       "        [-0.0349,  0.1682,  0.1669, -0.1522, -0.3319, -0.0587,  0.0477, -0.2973,\n",
       "         -0.1834, -0.1772]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2e2ba",
   "metadata": {},
   "source": [
    "**Advantage**: A custom block is more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8733d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fixedhiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772988f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0256, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = fixedhiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0094107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to mix various blocks and layers together.\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c09a74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0564, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), fixedhiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eddb5",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "1. Layers are blocks.\n",
    "2. Many layers can comprise a block.\n",
    "3. Many blocks can comprise a block.\n",
    "4. A block can contain code.\n",
    "5. Blocks take care of lots of housekeeping, including parameter initialization and backpropa- gation.\n",
    "6. Sequential concatenations of layers and blocks are handled by the **Sequential block**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4522839",
   "metadata": {},
   "source": [
    "## Parameter Management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b538b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3573],\n",
       "        [0.3759]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given an MLP with one hidden layer as an example\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand((2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c9073",
   "metadata": {},
   "source": [
    "**Parameter Access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b14192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0926,  0.0126, -0.0264,  0.1643,  0.1994, -0.1117,  0.0232,  0.1997]])), ('bias', tensor([0.2834]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b50558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2834], requires_grad=True)\n",
      "tensor([0.2834])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f0f0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d7f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7d3f314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2834])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4b693a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2521ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2290],\n",
       "        [-0.2290]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66949ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7b8b984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4816, -0.0984,  0.3902,  0.0649,  0.3528, -0.4240,  0.2899,  0.4091])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82b96b",
   "metadata": {},
   "source": [
    "**Parameter Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaec4df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0089, -0.0041, -0.0040, -0.0080]), tensor(0.))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c846557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcf6e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0122, -0.0154,  0.0068, -0.0022]) tensor(0.)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.]) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# use differnt initial methods in different blocks\n",
    "net[0].apply(init_normal)\n",
    "net[2].apply(init_constant)\n",
    "\n",
    "print(net[0].weight.data[0], net[0].bias.data[0])\n",
    "print(net[2].weight.data[0], net[2].bias.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "882ec13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init ('0.weight', Parameter containing:\n",
      "tensor([[ 0.0122, -0.0154,  0.0068, -0.0022],\n",
      "        [-0.0008,  0.0030, -0.0155, -0.0128],\n",
      "        [ 0.0105, -0.0074, -0.0074, -0.0075],\n",
      "        [ 0.0111,  0.0068, -0.0126,  0.0121],\n",
      "        [-0.0134,  0.0055, -0.0038, -0.0015],\n",
      "        [-0.0054, -0.0041, -0.0007,  0.0003],\n",
      "        [-0.0037,  0.0063, -0.0077, -0.0130],\n",
      "        [ 0.0119, -0.0062,  0.0074,  0.0257]], requires_grad=True)) ('0.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)) ('2.weight', Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)) ('2.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n",
      "Init ('0.weight', Parameter containing:\n",
      "tensor([[ 6.1249,  0.0000, -9.7135, -6.3069],\n",
      "        [ 6.1423, -0.0000,  0.0000,  0.0000],\n",
      "        [ 9.1035,  0.0000,  0.0000, -6.7997],\n",
      "        [ 0.0000, -0.0000,  0.0000,  7.4413],\n",
      "        [-0.0000, -0.0000,  0.0000, -8.2895],\n",
      "        [ 9.0873,  9.9752, -0.0000, -9.5554],\n",
      "        [ 0.0000, -5.1721,  0.0000,  0.0000],\n",
      "        [ 6.2516,  6.0649, -5.5443, -0.0000]], requires_grad=True)) ('0.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)) ('2.weight', Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)) ('2.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.1249,  0.0000, -9.7135, -6.3069],\n",
       "        [ 6.1423, -0.0000,  0.0000,  0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom initialization\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print('Init', *[(name, param) for name, param in net.named_parameters()])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        \n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87610a81",
   "metadata": {},
   "source": [
    "**Tied Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa9e57c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n",
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# we need to give a shared layer name so that we can refer to its parameters\n",
    "shared = nn.Linear(8, 8)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1))\n",
    "net(X)\n",
    "\n",
    "# check whether the parameters are shared\n",
    "print(net[2].weight.data == net[4].weight.data)\n",
    "net[2].weight.data[0] = 100\n",
    "# check whether the shared parameters update together\n",
    "print(net[2].weight.data == net[4].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2c61b",
   "metadata": {},
   "source": [
    "## Custom Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c16ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Layers without parameters\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e2d837f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a86f348d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6566e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2185f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers with parameters\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units, ))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b760860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.6702,  0.6636, -1.7579],\n",
       "        [ 1.3084, -0.5721, -1.6945],\n",
       "        [-0.3314,  0.2430,  0.4385],\n",
       "        [ 0.0226, -1.1508, -1.7520],\n",
       "        [-1.6324,  0.3254, -1.1020]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebc06dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.1374],\n",
       "        [0.0000, 4.8473, 2.8738]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.randn(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8fe01d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [3.7473],\n",
       "        [0.0000],\n",
       "        [8.2525],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.randn(5, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d70d05",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "1. We can design custom layers via the basic layer class. This allows us to define flexible new layers that behave differently from any existing layers in the library.\n",
    "2. Once defined, custom layers can be invoked in arbitrary contexts and architectures. \n",
    "3. Layers can have local parameters, which can be created through built-in functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c8787",
   "metadata": {},
   "source": [
    "## File I/O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fba864b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ff768e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d4ffc",
   "metadata": {},
   "source": [
    "**Loading and Saving Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3f1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7815e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7f992f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01214ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba548d5",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "1. The save and load functions can be used to perform file I/O for tensor objects.\n",
    "2. We can save and load the entire sets of parameters for a network via a parameter dictionary. \n",
    "3. Saving the architecture has to be done in code rather than in parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
